{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sandbox.celine_knowledge_graph import *\n",
    "# from sandbox.spacy_experiments import *\n",
    "# from sandbox.neuralcoref_experiments import *\n",
    "# from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1357aaf98>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import spacy\n",
    "import neuralcoref\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    s = s.split()\n",
    "    # remove everything from \"-lrb-\" to \"-rrb-\"\n",
    "    s2 = []\n",
    "    in_paren = False\n",
    "    for ixw, w in enumerate(s):\n",
    "        if(w==\"-lrb-\"):\n",
    "            in_paren=True\n",
    "        elif(w=='-rrb-'):\n",
    "            in_paren=False\n",
    "        elif(w==\"-lsb-\" or w==\"-rsb-\"):\n",
    "            continue\n",
    "        elif(ixw <= len(s)-2 and w==\"new\" and s[ixw+1]==\":\"):\n",
    "            continue\n",
    "        elif(ixw <= len(s)-1 and w==\":\" and s[ixw-1]==\"new\"):\n",
    "            continue\n",
    "        elif(len(w) > 1 and w[0] == '\\''):\n",
    "            s2[-1] = s2[-1]+w\n",
    "        elif not in_paren and not (w == '<t>' or w == '</t>'):\n",
    "            s2.append(w)\n",
    "        \n",
    "    return ' '.join(s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraph:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.relations = list()\n",
    "        self.noun_threshold = 0.9\n",
    "        self.verb_threshold = 0.9\n",
    "        self.weak_threshold = 0.5\n",
    "        self.entailment = 0\n",
    "        self.entailment_dissimilar_verbs = 0.5\n",
    "        self.dissimilar_verbs = 1\n",
    "        self.missing_dependencies = 2\n",
    "        self.contradiction = 3\n",
    "\n",
    "    # ==========================================\n",
    "    # 1) adding to KnowledgeGraph relations \n",
    "    # ==========================================\n",
    "    def add_verb(self, verb):\n",
    "        self.relations.append(self.get_relation(verb))\n",
    "        \n",
    "    ##### extracting relations from sentence #####\n",
    "    def get_relation(self, verb):\n",
    "        # get all equivalent verbs\n",
    "        verb_cluster = self.get_verb_cluster(verb)\n",
    "        actors = []\n",
    "        acteds = []\n",
    "        \n",
    "        # get all actors/acteds of verbs in equivalencies\n",
    "        for verb in verb_cluster:\n",
    "            actors += self.get_actors(verb)\n",
    "            acteds += self.get_acteds(verb)\n",
    "        return verb_cluster, actors, acteds\n",
    "    \n",
    "    # =========================================\n",
    "    # 2) looks through verb's children for\n",
    "    # verb equivalencies (xcomp)\n",
    "    # =========================================\n",
    "    def get_verb_cluster(self, verb):\n",
    "        verb_cluster = [verb]\n",
    "        for child in verb.children:\n",
    "            if child.dep_ == \"xcomp\":# or child.dep_ == \"ccomp\":\n",
    "                verb_cluster.append(child)\n",
    "        return verb_cluster\n",
    "        \n",
    "    def get_actors(self, verb):\n",
    "        actors = []\n",
    "        for child in verb.children:\n",
    "            # child is a nominative subject\n",
    "            if child.dep_ == \"nsubj\":\n",
    "                actors.append(child)\n",
    "            # child is something like \"by\"\n",
    "            elif child.dep_ == \"agent\":  \n",
    "                # passive, look for true actor\n",
    "                for grandchild in child.children:\n",
    "                    if grandchild.dep_ == \"pobj\":\n",
    "                        actors.append(grandchild)\n",
    "        return actors\n",
    "\n",
    "    def get_acteds(self, verb):\n",
    "        acteds = []\n",
    "        for child in verb.children:\n",
    "            #child is direct object or passive subject\n",
    "            if child.dep_ == \"dobj\" or child.dep_ == \"nsubjpass\":\n",
    "                acteds.append(child)\n",
    "        return acteds\n",
    "\n",
    "    # =========================================\n",
    "    # 3) checking hypothesis relation against \n",
    "    # premise's KnowledgeGraph relations\n",
    "    # =========================================\n",
    "    def query_relation(self, hypothesis):\n",
    "        missing_dependencies = []\n",
    "        contradiction = []\n",
    "        for premise in self.relations:\n",
    "            r = self.implied_relation(premise, hypothesis)\n",
    "\n",
    "            # once we find that hypothesis is contained, accept this relation as verified\n",
    "            # if the verb similarity is too low, we make note of this but still mark it as entailed\n",
    "            if r[0] == self.entailment:\n",
    "                return r[0], [(premise,r[1])]\n",
    "            elif r[0] == self.missing_dependencies:\n",
    "                missing_dependencies.append((premise, r[1]))\n",
    "            elif r[0] == self.contradiction:\n",
    "                contradiction.append((premise, r[1]))\n",
    "        if len(contradiction) > 0:\n",
    "            return self.contradiction, contradiction\n",
    "        return self.missing_dependencies, missing_dependencies\n",
    "    \n",
    "    # check if a hypothesis is verified by a premise \n",
    "    # returns (result, proof)\n",
    "    def implied_relation(self, premise, hypothesis):\n",
    "        # premise[0] and hypothesis[0] is a list (verb cluster)\n",
    "        verb_similarity, best_pair = self.verb_same(premise[0], hypothesis[0])\n",
    "        if verb_similarity < self.verb_threshold:\n",
    "            return self.dissimilar_verbs, hypothesis\n",
    "        \n",
    "        # check setminus of premise \\ hypothesis\n",
    "        actor_actor = self.noun_intersect_setminus(premise[1], hypothesis[1])\n",
    "        acted_acted = self.noun_intersect_setminus(premise[2], hypothesis[2])\n",
    "        actor_acted = self.noun_intersect_setminus(premise[1], hypothesis[2])\n",
    "        acted_actor = self.noun_intersect_setminus(premise[2], hypothesis[1])\n",
    "\n",
    "        contained_deps = actor_actor[0] + acted_acted[0]\n",
    "        missing_deps = actor_actor[1] + acted_acted[1]\n",
    "        contradiction_deps = actor_acted[0] + acted_actor[0]\n",
    "        \n",
    "        if len(missing_deps) == 0:\n",
    "            return self.entailment, (\"verb similarity:\", verb_similarity,\n",
    "                    \"contained dependences:\", contained_deps)\n",
    "        if len(contradiction_deps) > 0:\n",
    "            return self.contradiction, (\"verb similarity:\", verb_similarity,\n",
    "                    \"contradictory dependences:\", contradiction_deps)\n",
    "        return self.missing_dependencies, (\"verb similarity:\",\n",
    "                verb_similarity, \"missing dependencies:\", missing_deps)\n",
    "\n",
    "    \n",
    "    # ========================\n",
    "    # verb helper functions\n",
    "    # ========================\n",
    "    # v1 comes from premise/source, v2 comes from hypothesis/output\n",
    "    def verb_same(self, v1_cluster, v2_cluster):\n",
    "        maximum_similarity = 0\n",
    "        maximum_pair = None\n",
    "        for v1 in v1_cluster:\n",
    "            for v2 in v2_cluster:\n",
    "                similarity = v1.similarity(v2)\n",
    "                if(similarity > maximum_similarity):\n",
    "                    maximum_similarity = similarity\n",
    "                    maximum_pair = v1, v2\n",
    "        return maximum_similarity, maximum_pair\n",
    "    \n",
    "\n",
    "    # ========================\n",
    "    # noun helper functions\n",
    "    # ========================\n",
    "    def noun_intersect_setminus(self, supset, subset):\n",
    "        contained_nouns = []\n",
    "        missing_nouns = []\n",
    "        for n in subset:\n",
    "            contained = False\n",
    "            for n2 in supset:\n",
    "                r = self.noun_same(n, n2)\n",
    "                if verbose:\n",
    "                    print(n, n2, r)\n",
    "                if r[0]:\n",
    "                    contained = True\n",
    "                    contained_nouns.append((n, n2, r[1]))\n",
    "                    continue\n",
    "            if not contained:\n",
    "                missing_nouns.append(n)\n",
    "        return contained_nouns, missing_nouns\n",
    "\n",
    "    def noun_same(self, n1, n2):\n",
    "        tokens1 = self.get_valid_cluster_tokens(n1)\n",
    "        tokens2 = self.get_valid_cluster_tokens(n2)\n",
    "        if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "            tokens1 = self.get_valid_cluster_tokens(n1, True)\n",
    "            tokens2 = self.get_valid_cluster_tokens(n2, True)\n",
    "        maximum_similarity = 0\n",
    "        maximum_pair = None\n",
    "        for token1 in tokens1:\n",
    "            for token2 in tokens2:\n",
    "                token_similarity = token1.similarity(token2)\n",
    "                if token_similarity > maximum_similarity:\n",
    "                    maximum_similarity = token_similarity\n",
    "                    maximum_pair = token1, token2\n",
    "        if maximum_similarity > self.noun_threshold:\n",
    "            return True, (\"best match:\", maximum_similarity, maximum_pair)\n",
    "        return False, (\"best match:\", maximum_similarity, maximum_pair)\n",
    "    \n",
    "    def get_valid_cluster_tokens(self, noun, use_generic=False):\n",
    "        tokens = list()\n",
    "        if (noun.pos_ == 'PRON' or noun.pos_ == 'DET') and noun.head.dep_ == 'relcl':\n",
    "            # the head is the verb of the relative clause\n",
    "            # the head of the verb should be the noun this thing refers to\n",
    "            if verbose:\n",
    "                print(\"found relative clause, replacing\", noun, \"with\", noun.head.head)\n",
    "            noun = noun.head.head\n",
    "        for cluster in noun._.coref_clusters:\n",
    "            for span in cluster:\n",
    "                for token in span:\n",
    "                    if use_generic or not self.is_generic(token):\n",
    "                        if verbose and self.is_generic(token):\n",
    "                            print(colored(\"warning:\", \"yellow\"), \"using generic token\", noun)\n",
    "                        tokens.append(token)\n",
    "        if len(tokens) == 0:\n",
    "            if use_generic or not self.is_generic(noun):\n",
    "                if verbose and self.is_generic(noun):\n",
    "                    print(colored(\"warning:\", \"yellow\"), \"using generic token\", noun)\n",
    "                tokens.append(noun)\n",
    "        return tokens \n",
    "\n",
    "    def is_generic(self, token):\n",
    "        return token.pos_ == \"PRON\" or token.pos_ == \"DET\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(src, gen):\n",
    "#     print(\"source:\", src_line[:100])\n",
    "#     print(\"summary:\", gen_line[:100])\n",
    "    src = nlp(src)\n",
    "    gen = nlp(gen)\n",
    "#     print(\"clusters:\", src._.coref_clusters)\n",
    "    kg = KnowledgeGraph()\n",
    "\n",
    "    # put all actors/acteds for each verb into knowledge graph\n",
    "    for ixt, token in enumerate(src):\n",
    "        if token.pos_ == \"VERB\":\n",
    "            kg.add_verb(token)\n",
    "    important_relations = []\n",
    "    contained = 0\n",
    "    missing = 0\n",
    "    contradiction = 0\n",
    "    total = 0\n",
    "    \n",
    "    for token in gen:\n",
    "        # ignore xcomp verbs \"tried TO EAT\" since will later be added to verb cluster\n",
    "        # still adds was/has/is/aux verbs though\n",
    "        if token.pos_ == \"VERB\" and not(token.dep_=='xcomp'):# or token.dep_=='ccomp'):\n",
    "            relation = kg.get_relation(token)\n",
    "            # skip those relations with no actors/acteds\n",
    "            if (len(relation[1]) + len(relation[2]) == 0):\n",
    "                continue\n",
    "            \n",
    "            total += 1\n",
    "            r = kg.query_relation(relation)\n",
    "            if r[0] == kg.entailment:\n",
    "                contained += 1\n",
    "                important_relations.append(('contained', relation, r[1]))\n",
    "                if(verbose):\n",
    "                    print(\"contained |\", relation, \"|\", r[1])\n",
    "#             elif r[0] == kg.entailment_dissimilar_verbs:\n",
    "#                 missing += 1\n",
    "#                 important_relations.append(('contained-noverb', relation, r[1]))\n",
    "#                 if(verbose):\n",
    "#                     print(\"contained-noverb |\", relation, \"|\", r[1])\n",
    "            elif r[0] == kg.missing_dependencies:\n",
    "                missing += 1\n",
    "                important_relations.append(('missing', relation, r[1]))\n",
    "                if(verbose):\n",
    "                    print(colored(\"missing\", \"yellow\"), \"|\", relation, \"|\", r[1])\n",
    "            elif r[0] == kg.contradiction:\n",
    "                contradiction += 1\n",
    "                important_relations.append(('contradiction', relation, r[1]))\n",
    "                if(verbose):\n",
    "                    print(colored(\"contradiction\", \"red\"), \"|\", relation, \"|\", r[1])\n",
    "    \n",
    "    important_relations = sorted(important_relations)\n",
    "    colored_src, colored_gen = visualize([word.text for word in src], [word.text for word in gen], important_relations)\n",
    "    \n",
    "    if total == 0:\n",
    "        return important_relations, (0.0, 0.0, 0.0), (colored_src, colored_gen)\n",
    "    return important_relations, (100.0 * contained / total, \n",
    "                                 100.0 * missing / total, \n",
    "                                 100.0 * contradiction / total), (colored_src, colored_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(src0, gen0, important_relations):\n",
    "    colors = {'contained':lambda text: '\\033[0;32m' + text + '\\033[0m', \n",
    "#               'contained-noverb':lambda text: '\\033[0;95m' + text + '\\033[0m', \n",
    "              'missing':lambda text: '\\033[0;33m' + text + '\\033[0m', \n",
    "              'contradiction':lambda text: '\\033[0;31m' + text + '\\033[0m'}\n",
    "\n",
    "    colored_src = src0\n",
    "    colored_gen = gen0\n",
    "    for order in ['missing', 'contained-noverb', 'contained', 'contradiction']:\n",
    "        for relation_tuple in important_relations:\n",
    "            result = relation_tuple[0]\n",
    "            relation = relation_tuple[1]\n",
    "            proof = relation_tuple[2]\n",
    "            if not(result == order):\n",
    "                continue\n",
    "            # color output doc\n",
    "            verbs = relation[0]\n",
    "            actors = relation[1]\n",
    "            acteds = relation[2]\n",
    "            for verb in verbs:\n",
    "                colored_gen[verb.i] = colors[result](verb.text)\n",
    "            for a in actors:\n",
    "                colored_gen[a.i] = colors[result](a.text)\n",
    "            for a in acteds:\n",
    "                colored_gen[a.i] = colors[result](a.text)\n",
    "\n",
    "            # color source doc\n",
    "            for p in proof:\n",
    "                for verb in p[0][0]:\n",
    "                    colored_src[verb.i] = colors[result](verb.text)\n",
    "                for a in p[0][1]:\n",
    "                    colored_src[a.i] = colors[result](a.text)\n",
    "                for a in p[0][2]:\n",
    "                    colored_src[a.i] = colors[result](a.text)\n",
    "\n",
    "    colored_src = ' '.join(colored_src)\n",
    "    colored_gen = ' '.join(colored_gen)\n",
    "\n",
    "    return colored_src, colored_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns average number of tokens copied = max copy length / unique phrases copied\n",
    "def avg_copy_length(src,gen):\n",
    "    src = src.split()\n",
    "    gen = gen.split()\n",
    "    substrings = {}\n",
    "    for ixgw,word in enumerate(gen):\n",
    "        substrings[ixgw] = []\n",
    "    \n",
    "    avg_length = 0\n",
    "    num_copied = 0\n",
    "    ixgw = 0\n",
    "    while(ixgw < len(gen)):\n",
    "        gen_word = gen[ixgw]\n",
    "        max_js = []\n",
    "        src_ixs = []\n",
    "        for ixsw, src_word in enumerate(src):\n",
    "            j = 0\n",
    "            while(ixgw+j <= len(gen) and ixsw+j <= len(src) and src[ixsw:ixsw+j] == gen[ixgw:ixgw+j]):\n",
    "                j += 1\n",
    "            if(len(max_js) == 0 or j > max_js[0]):\n",
    "                max_js = [j]\n",
    "                src_ixs = [ixsw]\n",
    "            elif(j == max_js[0]):\n",
    "                max_js.append(j)\n",
    "                src_ixs.append(ixsw)\n",
    "        substrings[ixgw] = ([gen[ixgw:ixgw+max_j-1] for max_j in max_js], src_ixs)\n",
    "        ixgw += 1\n",
    "        \n",
    "    for ixgw,gen_word in enumerate(gen):\n",
    "#         substr = substrings[ixgw][0]\n",
    "#         src_ix = substrings[ixgw][1]\n",
    "        contained = False\n",
    "        for src_ix in substrings[ixgw][1]:\n",
    "            if ixgw > 0 and src_ix-1 in substrings[ixgw-1][1]:\n",
    "                contained=True\n",
    "                break\n",
    "        \n",
    "        if not contained:\n",
    "            if(len(substrings[ixgw][0])>0):\n",
    "                num_copied += 1\n",
    "#                 print(substrings[ixgw])\n",
    "#                 print(len(substrings[ixgw][0][0]))\n",
    "                avg_length += len(substrings[ixgw][0][0])\n",
    "    \n",
    "    avg_length = 0 if num_copied == 0 else avg_length/num_copied\n",
    "    \n",
    "    return avg_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moses meredith (False, ('best match:', 0.3165538, (moses, ellie)))\n",
      "moses trey (False, ('best match:', 0.329055, (moses, trey)))\n",
      "moses ellie (False, ('best match:', 0.3165538, (moses, ellie)))\n",
      "\u001b[33mmissing\u001b[0m | ([be], [moses], []) | [(([be], [meredith], []), ('verb similarity:', 1.0, 'missing dependencies:', [moses])), (([asked, be], [trey], [ellie]), ('verb similarity:', 1.0, 'missing dependencies:', [moses])), (([be], [], []), ('verb similarity:', 1.0, 'missing dependencies:', [moses]))]\n",
      "trey mother (False, ('best match:', 0.07276874, (trey, mother)))\n",
      "\u001b[33mmissing\u001b[0m | ([was], [trey], []) | [(([was], [], []), ('verb similarity:', 1.0, 'missing dependencies:', [trey])), (([was], [], []), ('verb similarity:', 1.0, 'missing dependencies:', [trey])), (([was], [mother], []), ('verb similarity:', 1.0, 'missing dependencies:', [trey]))]\n",
      "helson helson (True, ('best match:', 1.0, (helson, helson)))\n",
      "staff staff (True, ('best match:', 1.0, (staff, staff)))\n",
      "staff helson (False, ('best match:', 0, None))\n",
      "helson staff (False, ('best match:', 0, None))\n",
      "contained | ([alerted], [helson], [staff]) | [(([alerted], [helson], [staff]), ('verb similarity:', 1.0, 'contained dependences:', [(helson, helson, ('best match:', 1.0, (helson, helson))), (staff, staff, ('best match:', 1.0, (staff, staff)))]))]\n",
      "trey trey (True, ('best match:', 1.0, (trey, trey)))\n",
      "contained | ([made], [trey], []) | [(([made], [trey], []), ('verb similarity:', 1.0, 'contained dependences:', [(trey, trey, ('best match:', 1.0, (trey, trey)))]))]\n",
      "===========================================================================================\n",
      "Src 2: college - bound basketball star asks girl with down syndrome to high school prom . pictures of the two during the ` ` prom - posal ' ' have gone viral . ( cnn ) he 's a blue chip college basketball recruit . she 's a high school freshman with down syndrome . at first glance trey moses and ellie \u001b[0;33mmeredith\u001b[0m could n't \u001b[0;33mbe\u001b[0m more different . but all that changed thursday when \u001b[0;33mtrey\u001b[0m \u001b[0;33masked\u001b[0m \u001b[0;33mellie\u001b[0m to \u001b[0;33mbe\u001b[0m his prom date . trey -- a star on eastern high school 's basketball team in louisville , kentucky , who 's headed to play college ball next year at ball state -- \u001b[0;33mwas\u001b[0m originally going to take his girlfriend to eastern 's prom . so why is he taking ellie instead ? ` ` she 's great ... she listens and she 's easy to talk to ' ' he said . \u001b[0;32mtrey\u001b[0m \u001b[0;32mmade\u001b[0m the prom - posal ( yes , that 's what they are calling invites to prom these days ) in the gym during ellie 's p.e . class . trina \u001b[0;32mhelson\u001b[0m , a teacher at eastern , \u001b[0;32malerted\u001b[0m the school 's newspaper \u001b[0;32mstaff\u001b[0m to the prom - posal and posted photos of trey and ellie on twitter that have gone viral . she \u001b[0;33mwas\u001b[0m n't surpristed by trey 's actions . ` ` that 's the kind of person trey is , '' she said . to help make sure she said yes , trey entered the gym armed with flowers and a poster that read ` ` let 's party like it 's 1989 , '' a reference to the latest album by taylor swift , ellie 's favorite singer . trey also got the ok from ellie 's parents the night before via text . they were thrilled . ` ` you just feel numb to those moments raising a special needs child , '' said darla meredith , ellie 's mom . ` ` you first feel the need to protect and then to overprotect . ' ' darla meredith said ellie has struggled with friendships since elementary school , but a special program at eastern called best buddies had made things easier for her . she said best buddies cultivates friendships between students with and without developmental disabilities and prevents students like ellie from feeling isolated and left out of social functions . ` ` i guess around middle school is when kids started to care about what others thought , '' she said , but ` ` this school , this year has been a relief . ' ' trey 's future coach at ball state , james whitford , said he felt great about the prom - posal , noting that trey , whom he 's known for a long time , often works with other kids . trey 's \u001b[0;33mmother\u001b[0m , shelly moses , \u001b[0;33mwas\u001b[0m also proud of her son . ` ` it 's exciting to bring awareness to a good cause , '' she said . ` ` trey has worked pretty hard , and he 's a good son . ' ' both trey and ellie have a lot of planning to do . trey is looking to take up special education as a college major , in addition to playing basketball in the fall . as for ellie , she ca n't stop thinking about prom . ` ` ellie ca n't wait to go dress shopping ' ' her mother said . ` ` because i 've only told about a million people ! '' ellie interjected .\n",
      "===========================================================================================\n",
      "Summary 2: trey \u001b[0;33mmoses\u001b[0m and ellie meredith could n't \u001b[0;33mbe\u001b[0m more different . \u001b[0;33mtrey\u001b[0m \u001b[0;33mwas\u001b[0m a star on eastern high school 's basketball team in louisville , kentucky . trina \u001b[0;32mhelson\u001b[0m , a teacher at eastern , \u001b[0;32malerted\u001b[0m the school 's newspaper \u001b[0;32mstaff\u001b[0m . \u001b[0;32mtrey\u001b[0m \u001b[0;32mmade\u001b[0m the prom - posal 's\n",
      "Score: (50.0, 50.0, 0.0)\n",
      "Avg copy length: 7.333333333333333\n",
      "===========================================================================================\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "src_path = \"data/fast-abs-RL/articles.txt\"\n",
    "tgt_path = \"data/fast-abs-RL/reference.txt\"\n",
    "gen_path = \"data/fast-abs-RL/decoded.txt\"\n",
    "\n",
    "line_num = 2\n",
    "scores = []\n",
    "src_lines = []\n",
    "tgt_lines = []\n",
    "gen_lines = []\n",
    "with open(src_path) as src:\n",
    "    with open(tgt_path) as tgt:\n",
    "        with open(gen_path) as gen:\n",
    "            for i, (orig_src_line, orig_tgt_line, gen_line) in enumerate(zip(src, tgt, gen)):\n",
    "    #             if i < 30:\n",
    "    #                 continue\n",
    "                if line_num > 0 and not i == line_num:\n",
    "                    continue\n",
    "                if line_num == 0 and i >= 40:\n",
    "                    break\n",
    "                orig_src_line = clean(orig_src_line)\n",
    "                orig_tgt_line = clean(orig_tgt_line)\n",
    "                src_line = orig_tgt_line + ' ' + orig_src_line\n",
    "                src_lines.append(orig_src_line)\n",
    "                tgt_lines.append(orig_tgt_line)\n",
    "                gen_line = clean(gen_line)\n",
    "                gen_lines.append(gen_line)\n",
    "                important_relations, score, (colored_src, colored_gen) = test(src_line, gen_line)\n",
    "                print(\"===========================================================================================\")\n",
    "                print(f\"Src {i}:\"%{i:i}, colored_src)\n",
    "                print(\"===========================================================================================\")\n",
    "                print(f\"Summary {i}:\"%{i:i}, colored_gen)\n",
    "                print(\"Score:\", score)\n",
    "                avg_length = avg_copy_length(orig_src_line, gen_line)\n",
    "                print(\"Avg copy length:\", avg_length)\n",
    "                print(\"===========================================================================================\")\n",
    "                print(\"===========================================================================================\")\n",
    "                scores.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( punct PUNCT cnn\n",
      "cnn ROOT PROPN cnn\n",
      ") punct PUNCT cnn\n",
      "he nsubj PRON 's\n",
      "'s ROOT VERB 's\n",
      "a det DET recruit\n",
      "blue amod ADJ chip\n",
      "chip compound NOUN recruit\n",
      "college compound NOUN basketball\n",
      "basketball compound NOUN recruit\n",
      "recruit attr NOUN 's\n",
      ". punct PUNCT 's\n",
      "she nsubj PRON 's\n",
      "'s ROOT VERB 's\n",
      "a det DET freshman\n",
      "high amod ADJ school\n",
      "school compound NOUN freshman\n",
      "freshman attr NOUN 's\n",
      "with prep ADP freshman\n",
      "down amod ADJ syndrome\n",
      "syndrome pobj NOUN with\n",
      ". punct PUNCT 's\n",
      "at prep ADP be\n",
      "first amod ADJ glance\n",
      "glance compound NOUN moses\n",
      "trey compound NOUN moses\n",
      "moses pobj PROPN at\n",
      "and cc CCONJ moses\n",
      "ellie compound PROPN meredith\n",
      "meredith nsubj NOUN be\n",
      "could aux VERB be\n",
      "n't neg ADV be\n",
      "be ROOT VERB be\n",
      "more advmod ADV different\n",
      "different acomp ADJ be\n",
      ". punct PUNCT be\n",
      "but cc CCONJ all\n",
      "all ROOT DET all\n",
      "that nsubj DET changed\n",
      "changed relcl VERB all\n",
      "thursday npadvmod NOUN changed\n",
      "when advmod ADV asked\n",
      "trey nsubj NOUN asked\n",
      "asked advcl VERB changed\n",
      "ellie dobj NOUN asked\n",
      "to aux PART be\n",
      "be xcomp VERB asked\n",
      "his poss DET date\n",
      "prom compound NOUN date\n",
      "date attr NOUN be\n",
      ". punct PUNCT all\n",
      "trey nsubj NOUN going\n",
      "-- punct PUNCT trey\n",
      "a det DET star\n",
      "star appos NOUN trey\n",
      "on prep ADP star\n",
      "eastern amod ADJ school\n",
      "high amod ADJ school\n",
      "school poss NOUN team\n",
      "'s case PART school\n",
      "basketball compound NOUN team\n",
      "team pobj NOUN on\n",
      "in prep ADP team\n",
      "louisville pobj NOUN in\n",
      ", punct PUNCT louisville\n",
      "kentucky conj NOUN louisville\n",
      ", punct PUNCT kentucky\n",
      "who nsubjpass PRON headed\n",
      "'s auxpass VERB headed\n",
      "headed relcl VERB star\n",
      "to aux PART play\n",
      "play advcl VERB headed\n",
      "college compound NOUN ball\n",
      "ball dobj NOUN play\n",
      "next amod ADJ year\n",
      "year npadvmod NOUN play\n",
      "at prep ADP play\n",
      "ball compound NOUN state\n",
      "state pobj NOUN at\n",
      "-- punct PUNCT trey\n",
      "was aux VERB going\n",
      "originally advmod ADV going\n",
      "going ROOT VERB going\n",
      "to aux PART take\n",
      "take xcomp VERB going\n",
      "his poss DET girlfriend\n",
      "girlfriend dobj NOUN take\n",
      "to prep ADP take\n",
      "eastern poss PROPN prom\n",
      "'s case PART eastern\n",
      "prom pobj NOUN to\n",
      ". punct PUNCT going\n",
      "so advmod ADV taking\n",
      "why advmod ADV taking\n",
      "is aux VERB taking\n",
      "he nsubj PRON taking\n",
      "taking ROOT VERB taking\n",
      "ellie dobj NOUN taking\n",
      "instead advmod ADV taking\n",
      "? punct PUNCT taking\n",
      "` punct PUNCT 's\n",
      "` punct PUNCT 's\n",
      "she nsubj PRON 's\n",
      "'s ROOT VERB 's\n",
      "great acomp ADJ 's\n",
      "... punct PUNCT 's\n",
      "she nsubj PRON listens\n",
      "listens ccomp VERB said\n",
      "and cc CCONJ listens\n",
      "she nsubj PRON 's\n",
      "'s conj VERB listens\n",
      "easy acomp ADJ 's\n",
      "to aux PART talk\n",
      "talk xcomp VERB 's\n",
      "to prep ADP talk\n",
      "' punct PUNCT 's\n",
      "' punct PUNCT said\n",
      "he nsubj PRON said\n",
      "said ROOT VERB said\n",
      ". punct PUNCT said\n",
      "trey nsubj NOUN made\n",
      "made ROOT VERB made\n",
      "the det DET posal\n",
      "prom compound NOUN posal\n",
      "- punct PUNCT posal\n",
      "posal ccomp NOUN made\n",
      "( punct PUNCT 's\n",
      "yes intj INTJ 's\n",
      ", punct PUNCT 's\n",
      "that nsubj DET 's\n",
      "'s parataxis VERB posal\n",
      "what dobj PRON calling\n",
      "they nsubj PRON calling\n",
      "are aux VERB calling\n",
      "calling ccomp VERB 's\n",
      "invites oprd NOUN calling\n",
      "to aux PART prom\n",
      "prom xcomp VERB invites\n",
      "these det DET days\n",
      "days npadvmod NOUN prom\n",
      ") punct PUNCT prom\n",
      "in prep ADP calling\n",
      "the det DET gym\n",
      "gym pobj NOUN in\n",
      "during prep ADP calling\n",
      "ellie poss PROPN class\n",
      "'s case PART ellie\n",
      "p.e subtok PROPN .\n",
      ". punct PUNCT class\n",
      "class pobj NOUN during\n",
      ". punct PUNCT made\n",
      "trina compound PROPN helson\n",
      "helson nsubj NOUN alerted\n",
      ", punct PUNCT helson\n",
      "a det DET teacher\n",
      "teacher appos NOUN helson\n",
      "at prep ADP teacher\n",
      "eastern amod ADJ at\n",
      ", punct PUNCT alerted\n",
      "alerted ROOT VERB alerted\n",
      "the det DET school\n",
      "school poss NOUN staff\n",
      "'s case PART school\n",
      "newspaper compound NOUN staff\n",
      "staff dobj NOUN alerted\n",
      "to prep ADP alerted\n",
      "the det DET posal\n",
      "prom compound NOUN posal\n",
      "- punct PUNCT posal\n",
      "posal pobj NOUN to\n",
      "and cc CCONJ alerted\n",
      "posted conj VERB alerted\n",
      "photos dobj NOUN posted\n",
      "of prep ADP photos\n",
      "trey pobj NOUN of\n",
      "and cc CCONJ trey\n",
      "ellie conj NOUN trey\n",
      "on prep ADP ellie\n",
      "twitter pobj NOUN on\n",
      "that nsubj DET gone\n",
      "have aux VERB gone\n",
      "gone relcl VERB twitter\n",
      "viral acomp ADJ gone\n",
      ". punct PUNCT alerted\n",
      "she nsubjpass PRON surpristed\n",
      "was auxpass VERB surpristed\n",
      "n't neg ADV surpristed\n",
      "surpristed ROOT VERB surpristed\n",
      "by agent ADP surpristed\n",
      "trey poss NOUN actions\n",
      "'s case PART trey\n",
      "actions pobj NOUN by\n",
      ". punct PUNCT surpristed\n",
      "` punct PUNCT 's\n",
      "` punct PUNCT 's\n",
      "that nsubj DET 's\n",
      "'s ccomp VERB said\n",
      "the det DET kind\n",
      "kind nsubj NOUN is\n",
      "of prep ADP kind\n",
      "person compound NOUN trey\n",
      "trey pobj NOUN of\n",
      "is ccomp VERB 's\n",
      ", punct PUNCT said\n",
      "'' punct PUNCT said\n",
      "she nsubj PRON said\n",
      "said ROOT VERB said\n",
      ". punct PUNCT said\n",
      "to aux PART help\n",
      "help advcl VERB entered\n",
      "make xcomp VERB help\n",
      "sure acomp ADJ make\n",
      "she nsubj PRON said\n",
      "said ccomp VERB sure\n",
      "yes intj INTJ said\n",
      ", punct PUNCT entered\n",
      "trey nsubj NOUN entered\n",
      "entered ROOT VERB entered\n",
      "the det DET gym\n",
      "gym dobj NOUN entered\n",
      "armed acl VERB gym\n",
      "with prep ADP armed\n",
      "flowers pobj NOUN with\n",
      "and cc CCONJ flowers\n",
      "a det DET poster\n",
      "poster conj NOUN flowers\n",
      "that nsubj DET read\n",
      "read relcl VERB poster\n",
      "` punct PUNCT read\n",
      "` punct PUNCT let\n",
      "let dep VERB read\n",
      "'s nsubj PRON party\n",
      "party ccomp NOUN let\n",
      "like mark ADP 's\n",
      "it nsubj PRON 's\n",
      "'s advcl VERB let\n",
      "1989 attr NUM 's\n",
      ", punct PUNCT entered\n",
      "'' punct PUNCT reference\n",
      "a det DET reference\n",
      "reference appos NOUN gym\n",
      "to prep ADP reference\n",
      "the det DET album\n",
      "latest amod ADJ album\n",
      "album pobj NOUN to\n",
      "by prep ADP reference\n",
      "taylor compound NOUN swift\n",
      "swift pobj NOUN by\n",
      ", punct PUNCT swift\n",
      "ellie poss PROPN singer\n",
      "'s case PART ellie\n",
      "favorite amod ADJ singer\n",
      "singer appos NOUN swift\n",
      ". punct PUNCT entered\n",
      "trey nsubj NOUN got\n",
      "also advmod ADV got\n",
      "got ROOT VERB got\n",
      "the det DET ok\n",
      "ok dobj NOUN got\n",
      "from prep ADP ok\n",
      "ellie poss PROPN parents\n",
      "'s case PART ellie\n",
      "parents pobj NOUN from\n",
      "the det DET night\n",
      "night npadvmod NOUN got\n",
      "before advmod ADP night\n",
      "via prep ADP night\n",
      "text pobj NOUN via\n",
      ". punct PUNCT got\n",
      "they nsubj PRON were\n",
      "were ROOT VERB were\n",
      "thrilled acomp VERB were\n",
      ". punct PUNCT were\n",
      "` punct PUNCT feel\n",
      "` punct PUNCT feel\n",
      "you nsubj PRON feel\n",
      "just advmod ADV feel\n",
      "feel ccomp VERB said\n",
      "numb acomp ADJ feel\n",
      "to prep ADP numb\n",
      "those det DET moments\n",
      "moments pobj NOUN to\n",
      "raising acl VERB moments\n",
      "a det DET child\n",
      "special amod ADJ needs\n",
      "needs compound NOUN child\n",
      "child dobj NOUN raising\n",
      ", punct PUNCT said\n",
      "'' punct PUNCT said\n",
      "said ROOT VERB said\n",
      "darla compound PROPN meredith\n",
      "meredith nsubj NOUN said\n",
      ", punct PUNCT meredith\n",
      "ellie poss PROPN mom\n",
      "'s case PART ellie\n",
      "mom appos NOUN meredith\n",
      ". punct PUNCT said\n",
      "` punct PUNCT feel\n",
      "` punct PUNCT feel\n",
      "you nsubj PRON feel\n",
      "first advmod ADV feel\n",
      "feel ROOT VERB feel\n",
      "the det DET need\n",
      "need dobj NOUN feel\n",
      "to aux PART protect\n",
      "protect acl VERB need\n",
      "and cc CCONJ protect\n",
      "then advmod ADV overprotect\n",
      "to aux PART overprotect\n",
      "overprotect conj VERB protect\n",
      ". punct PUNCT feel\n",
      "' punct PUNCT feel\n",
      "' punct PUNCT meredith\n",
      "darla compound PROPN meredith\n",
      "meredith nsubj NOUN said\n",
      "said ccomp VERB feel\n",
      "ellie nsubj PROPN struggled\n",
      "has aux VERB struggled\n",
      "struggled ccomp VERB said\n",
      "with prep ADP struggled\n",
      "friendships pobj NOUN with\n",
      "since prep ADP struggled\n",
      "elementary amod ADJ school\n",
      "school pobj NOUN since\n",
      ", punct PUNCT struggled\n",
      "but cc CCONJ struggled\n",
      "a det DET program\n",
      "special amod ADJ program\n",
      "program nsubj NOUN made\n",
      "at prep ADP program\n",
      "eastern amod ADJ at\n",
      "called acl VERB program\n",
      "best amod ADJ buddies\n",
      "buddies oprd NOUN called\n",
      "had aux VERB made\n",
      "made conj VERB struggled\n",
      "things nsubj NOUN easier\n",
      "easier ccomp ADJ made\n",
      "for prep ADP easier\n",
      "her pobj PRON for\n",
      ". punct PUNCT said\n",
      "she nsubj PRON said\n",
      "said ROOT VERB said\n",
      "best amod ADJ buddies\n",
      "buddies nsubj NOUN cultivates\n",
      "cultivates ccomp VERB said\n",
      "friendships dobj NOUN cultivates\n",
      "between prep ADP friendships\n",
      "students pobj NOUN between\n",
      "with prep ADP students\n",
      "and cc CCONJ with\n",
      "without conj ADP with\n",
      "developmental amod ADJ disabilities\n",
      "disabilities pobj NOUN without\n",
      "and cc CCONJ cultivates\n",
      "prevents conj VERB cultivates\n",
      "students dobj NOUN prevents\n",
      "like prep ADP students\n",
      "ellie pobj NOUN like\n",
      "from prep ADP prevents\n",
      "feeling pcomp VERB from\n",
      "isolated acomp ADJ feeling\n",
      "and cc CCONJ prevents\n",
      "left conj VERB prevents\n",
      "out prep ADP left\n",
      "of prep ADP out\n",
      "social amod ADJ functions\n",
      "functions pobj NOUN of\n",
      ". punct PUNCT said\n",
      "` punct PUNCT said\n",
      "` punct PUNCT guess\n",
      "i nsubj PRON guess\n",
      "guess ccomp VERB said\n",
      "around prep ADP guess\n",
      "middle amod ADJ school\n",
      "school pobj NOUN around\n",
      "is ccomp VERB guess\n",
      "when advmod ADV started\n",
      "kids nsubj NOUN started\n",
      "started advcl VERB is\n",
      "to aux PART care\n",
      "care xcomp VERB started\n",
      "about prep ADP care\n",
      "what dobj PRON thought\n",
      "others nsubj NOUN thought\n",
      "thought pcomp VERB about\n",
      ", punct PUNCT said\n",
      "'' punct PUNCT said\n",
      "she nsubj PRON said\n",
      "said ccomp VERB said\n",
      ", punct PUNCT said\n",
      "but cc CCONJ said\n",
      "` punct PUNCT been\n",
      "` punct PUNCT been\n",
      "this det DET school\n",
      "school npadvmod NOUN been\n",
      ", punct PUNCT been\n",
      "this det DET year\n",
      "year nsubj NOUN been\n",
      "has aux VERB been\n",
      "been conj VERB said\n",
      "a det DET relief\n",
      "relief attr NOUN been\n",
      ". punct PUNCT been\n",
      "' punct PUNCT been\n",
      "' punct PUNCT coach\n",
      "trey poss NOUN coach\n",
      "'s case PART trey\n",
      "future amod ADJ coach\n",
      "coach attr NOUN been\n",
      "at prep ADP coach\n",
      "ball compound NOUN state\n",
      "state pobj NOUN at\n",
      ", punct PUNCT coach\n",
      "james compound NOUN whitford\n",
      "whitford appos PROPN coach\n",
      ", punct PUNCT said\n",
      "said ROOT VERB said\n",
      "he nsubj PRON felt\n",
      "felt ccomp VERB said\n",
      "great acomp ADJ felt\n",
      "about prep ADP great\n",
      "the det DET posal\n",
      "prom compound NOUN posal\n",
      "- punct PUNCT posal\n",
      "posal pobj NOUN about\n",
      ", punct PUNCT felt\n",
      "noting advcl VERB felt\n",
      "that mark ADP works\n",
      "trey nsubj NOUN works\n",
      ", punct PUNCT trey\n",
      "whom dobj PRON known\n",
      "he nsubjpass PRON known\n",
      "'s auxpass VERB known\n",
      "known relcl VERB trey\n",
      "for prep ADP known\n",
      "a det DET time\n",
      "long amod ADJ time\n",
      "time pobj NOUN for\n",
      ", punct PUNCT works\n",
      "often advmod ADV works\n",
      "works ccomp VERB noting\n",
      "with prep ADP works\n",
      "other amod ADJ kids\n",
      "kids pobj NOUN with\n",
      ". punct PUNCT said\n",
      "trey poss PROPN mother\n",
      "'s case PART trey\n",
      "mother nsubj NOUN was\n",
      ", punct PUNCT mother\n",
      "shelly compound PROPN moses\n",
      "moses appos PROPN mother\n",
      ", punct PUNCT was\n",
      "was ROOT VERB was\n",
      "also advmod ADV was\n",
      "proud acomp ADJ was\n",
      "of prep ADP proud\n",
      "her poss DET son\n",
      "son pobj NOUN of\n",
      ". punct PUNCT was\n",
      "` punct PUNCT 's\n",
      "` punct PUNCT 's\n",
      "it nsubj PRON 's\n",
      "'s ccomp VERB said\n",
      "exciting acomp ADJ 's\n",
      "to aux PART bring\n",
      "bring xcomp VERB 's\n",
      "awareness dobj NOUN bring\n",
      "to prep ADP bring\n",
      "a det DET cause\n",
      "good amod ADJ cause\n",
      "cause pobj NOUN to\n",
      ", punct PUNCT said\n",
      "'' punct PUNCT said\n",
      "she nsubj PRON said\n",
      "said ROOT VERB said\n",
      ". punct PUNCT said\n",
      "` punct PUNCT worked\n",
      "` punct PUNCT trey\n",
      "trey nsubj NOUN worked\n",
      "has aux VERB worked\n",
      "worked ROOT VERB worked\n",
      "pretty advmod ADV hard\n",
      "hard advmod ADV worked\n",
      ", punct PUNCT worked\n",
      "and cc CCONJ worked\n",
      "he nsubj PRON 's\n",
      "'s conj VERB worked\n",
      "a det DET son\n",
      "good amod ADJ son\n",
      "son attr NOUN 's\n",
      ". punct PUNCT 's\n",
      "' punct PUNCT 's\n",
      "' punct PUNCT have\n",
      "both preconj DET trey\n",
      "trey nsubj NOUN have\n",
      "and cc CCONJ trey\n",
      "ellie conj PROPN trey\n",
      "have ROOT VERB have\n",
      "a det DET lot\n",
      "lot dobj NOUN have\n",
      "of prep ADP lot\n",
      "planning pobj VERB of\n",
      "to aux PART do\n",
      "do xcomp VERB planning\n",
      ". punct PUNCT have\n",
      "trey nsubj NOUN looking\n",
      "is aux VERB looking\n",
      "looking ROOT VERB looking\n",
      "to aux PART take\n",
      "take xcomp VERB looking\n",
      "up prt PART take\n",
      "special amod ADJ education\n",
      "education dobj NOUN take\n",
      "as prep ADP take\n",
      "a det DET major\n",
      "college compound NOUN major\n",
      "major pobj NOUN as\n",
      ", punct PUNCT take\n",
      "in prep ADP take\n",
      "addition pobj NOUN in\n",
      "to prep ADP addition\n",
      "playing pcomp VERB to\n",
      "basketball dobj NOUN playing\n",
      "in prep ADP playing\n",
      "the det DET fall\n",
      "fall pobj NOUN in\n",
      ". punct PUNCT looking\n",
      "as prep ADP stop\n",
      "for prep ADP as\n",
      "ellie pobj NOUN for\n",
      ", punct PUNCT stop\n",
      "she nsubj PRON stop\n",
      "ca aux VERB stop\n",
      "n't neg ADV stop\n",
      "stop ROOT VERB stop\n",
      "thinking xcomp VERB stop\n",
      "about prep ADP thinking\n",
      "prom pobj NOUN about\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". punct PUNCT stop\n",
      "` punct PUNCT wait\n",
      "` punct PUNCT wait\n",
      "ellie nsubj NOUN wait\n",
      "ca aux VERB wait\n",
      "n't neg ADV wait\n",
      "wait ROOT VERB wait\n",
      "to aux PART go\n",
      "go xcomp VERB wait\n",
      "dress compound NOUN shopping\n",
      "shopping dobj NOUN go\n",
      "' punct PUNCT wait\n",
      "' punct PUNCT wait\n",
      "her poss DET mother\n",
      "mother nsubj NOUN said\n",
      "said ROOT VERB said\n",
      ". punct PUNCT said\n",
      "` punct PUNCT `\n",
      "` punct PUNCT told\n",
      "because mark ADP told\n",
      "i nsubj PRON told\n",
      "'ve aux VERB told\n",
      "only advmod ADV told\n",
      "told ROOT VERB told\n",
      "about quantmod ADP million\n",
      "a quantmod DET million\n",
      "million nummod NUM people\n",
      "people dobj NOUN told\n",
      "! punct PUNCT told\n",
      "'' punct PUNCT told\n",
      "ellie nsubj PROPN interjected\n",
      "interjected ROOT VERB interjected\n",
      ". punct PUNCT interjected\n"
     ]
    }
   ],
   "source": [
    "for token in nlp(src_lines[0]):\n",
    "    print(token, token.dep_, token.pos_, token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trey compound VERB moses\n",
      "moses nsubj PROPN be\n",
      "and cc CCONJ moses\n",
      "ellie compound PROPN meredith\n",
      "meredith conj NOUN moses\n",
      "could aux VERB be\n",
      "n't neg ADV be\n",
      "be ROOT VERB be\n",
      "more advmod ADV different\n",
      "different acomp ADJ be\n",
      ". punct PUNCT be\n",
      "trey nsubj NOUN was\n",
      "was ROOT VERB was\n",
      "a det DET star\n",
      "star attr NOUN was\n",
      "on prep ADP star\n",
      "eastern amod ADJ school\n",
      "high amod ADJ school\n",
      "school poss NOUN team\n",
      "'s case PART school\n",
      "basketball compound NOUN team\n",
      "team pobj NOUN on\n",
      "in prep ADP team\n",
      "louisville pobj NOUN in\n",
      ", punct PUNCT louisville\n",
      "kentucky appos NOUN louisville\n",
      ". punct PUNCT was\n",
      "trina compound PROPN helson\n",
      "helson nsubj NOUN alerted\n",
      ", punct PUNCT helson\n",
      "a det DET teacher\n",
      "teacher appos NOUN helson\n",
      "at prep ADP teacher\n",
      "eastern amod ADJ at\n",
      ", punct PUNCT alerted\n",
      "alerted ROOT VERB alerted\n",
      "the det DET school\n",
      "school poss NOUN staff\n",
      "'s case PART school\n",
      "newspaper compound NOUN staff\n",
      "staff dobj NOUN alerted\n",
      ". punct PUNCT alerted\n",
      "trey nsubj NOUN made\n",
      "made ROOT VERB made\n",
      "the det DET posal\n",
      "prom compound NOUN posal\n",
      "- punct PUNCT posal\n",
      "posal ccomp NOUN made\n",
      "'s case PART posal\n"
     ]
    }
   ],
   "source": [
    "for token in nlp(gen_lines[0]):\n",
    "    print(token, token.dep_, token.pos_, token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.load(\"sandbox/scores.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(scores[:,0], scores[:,2], marker='.')\n",
    "plt.xlabel(\"% contained\")\n",
    "plt.ylabel(\"% contradiction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(gen_lines[0], tgt_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.41999999503200003,\n",
       "   'p': 0.45652173913043476,\n",
       "   'r': 0.3888888888888889},\n",
       "  'rouge-2': {'f': 0.18032786393845757,\n",
       "   'p': 0.20754716981132076,\n",
       "   'r': 0.15942028985507245},\n",
       "  'rouge-l': {'f': 0.4147252747249494,\n",
       "   'p': 0.45652173913043476,\n",
       "   'r': 0.3888888888888889}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(gen_lines[1], tgt_lines[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying BERT embeddings...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relations between iran and saudi arabia have always been thorny , but rarely has the state of affairs been as venomous as it is today',\n",
       " 'tehran and riyadh each point to the other as the main reason for much of the turmoil in the middle east',\n",
       " 'in its most recent incarnation , the iranian-saudi conflict by proxy has reached yemen in a spiral that both sides portray as climatic',\n",
       " \"for riyadh and its regional allies , the saudi military intervention in yemen -- `` operation decisive storm'' -- is the moment the sunni arab nation finally woke up to repel the expansion of shia-iranian influence\",\n",
       " \"for tehran and its regional allies -- including the houthi movement in yemen -- saudi arabia's actions are in defense of a retrogressive status quo order that is no longer tenable\",\n",
       " 'and yet both sides have good reasons to want to stop the yemeni crisis from spiraling out of control and evolving into an unwinnable war',\n",
       " 'when iranian president hassan rouhani was elected in june 2013 , he pledged to reach out to riyadh',\n",
       " \"he was up front and called tehran's steep deterioration of relations with the saudis over the last decade as one of the principal burdens on iranian foreign policy\",\n",
       " 'from lebanon and afghanistan to pakistan and the gaza strip , the iranian-saudi rivalry and conflict through proxy has been deep and costly',\n",
       " \"and yet despite rouhani's open pledge , profound differences over syria and iraq in particular have kept riyadh and tehran apart\",\n",
       " 'but if the questions of syria and iraq prevented a pause in hostilities , the saudi military intervention in yemen since late march has all but raised the stakes to unprecedentedly dangerous levels',\n",
       " 'unlike in syria and in iraq , the saudi military is now directly battling it out with iranian-backed rebels in yemen',\n",
       " \"while riyadh no doubt exaggerates tehran's role in the yemen crisis , its fingerprints are nonetheless evident\",\n",
       " \"`` iran provides financial support , weapons , training and intelligence to houthis ,'' gerald feierstein , a u.s. state department official and former yemen ambassador , told a congressional hearing last week\",\n",
       " '`` we believe that iran sees opportunities with the houthis to expand its influence in yemen and threaten saudi and gulf arab']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embedding = BertEmbedding()\n",
    "embed = bert_embedding(src_lines[0].split(' . '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relations',\n",
       " 'between',\n",
       " 'iran',\n",
       " 'and',\n",
       " 'saudi',\n",
       " 'arabia',\n",
       " 'have',\n",
       " 'always',\n",
       " 'been',\n",
       " 'thorny',\n",
       " ',',\n",
       " 'but',\n",
       " 'rarely',\n",
       " 'has',\n",
       " 'the',\n",
       " 'state',\n",
       " 'of',\n",
       " 'affairs',\n",
       " 'been',\n",
       " 'as',\n",
       " 'venomous']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(embed[0][1])):\n",
    "#     plt.scatter([embed[0][1][i][0]], [embed[0][1][i][1]], label=embed[0][0][i])\n",
    "# # plt.scatter([embed[0][1][i][0] for i in range(len(embed[0][1]))], \n",
    "# #             [embed[0][1][i][1] for i in range(len(embed[0][1]))])\n",
    "# # plt.scatter([embed[0][1][2][0]], [embed[0][1][2][1]])\n",
    "# # plt.scatter([embed[0][1][4][0]], [embed[0][1][4][1]])\n",
    "# # plt.scatter([embed[0][1][5][0]], [embed[0][1][5][1]])\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.376158"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(embed[0][1][4] - embed[0][1][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab file is not found. Downloading.\n",
      "Downloading /Users/cliang/.mxnet/models/book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
      "Downloading /Users/cliang/.mxnet/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip...\n"
     ]
    }
   ],
   "source": [
    "bert_embedding = BertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "with open(\"data/test.txt.src.tagged.shuf.400words\") as src:\n",
    "    with open(\"data/bottom_up_cnndm_015_threshold.out\") as gen:\n",
    "        for src_line0, gen_line0 in zip(src, gen):\n",
    "            if(i >= 8):\n",
    "                break\n",
    "#             print(src_line0)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = gen_line.split(' . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = bert_embedding([\"I ate a dog\", \"I ate a cat\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'ate', 'a', 'dog']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.3448873"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(result[1][1][3]- result[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9955516"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(result[1][1][1]- result[0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
