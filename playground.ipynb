{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sandbox.celine_knowledge_graph import *\n",
    "from sandbox.spacy_experiments import *\n",
    "from sandbox.neuralcoref_experiments import *\n",
    "from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x14824ceb8>"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import spacy\n",
    "import neuralcoref\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_src(s):\n",
    "    s = s.split()\n",
    "    # remove everything from \"-lrb-\" to \"-rrb-\"\n",
    "    s2 = []\n",
    "    in_paren = False\n",
    "    for ixw, w in enumerate(s):\n",
    "        if(w==\"-lrb-\"):\n",
    "            in_paren=True\n",
    "        elif(w=='-rrb-'):\n",
    "            in_paren=False\n",
    "        elif(w==\"-lsb-\" or w==\"-rsb-\"):\n",
    "            continue\n",
    "        elif(len(w) > 1 and w[0] == '\\''):\n",
    "            s2[-1] = s2[-1]+w\n",
    "        elif not in_paren and not (w == '<t>' or w == '</t>'):\n",
    "            s2.append(w)\n",
    "    return ' '.join(s2)\n",
    "\n",
    "def clean_gen(s):\n",
    "    s = s.split()\n",
    "    # remove everything from \"-lrb-\" to \"-rrb-\"\n",
    "    s2 = []\n",
    "    in_paren = False\n",
    "    for w in s:\n",
    "        if(w==\"-lrb-\"):\n",
    "            in_paren=True\n",
    "        elif(w=='-rrb-'):\n",
    "            in_paren=False\n",
    "        elif(w==\"-lsb-\" or w==\"-rsb-\"):\n",
    "            continue\n",
    "        elif(len(w) > 1 and w[0] == '\\''):\n",
    "            s2[-1] = s2[-1]+w\n",
    "        elif not in_paren and not(w == '<t>' or w == '</t>'):\n",
    "            s2.append(w)\n",
    "    return ' '.join(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraph:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.relations = list()\n",
    "        self.noun_threshold = 0.9\n",
    "        self.verb_threshold = 0.9\n",
    "        self.entailment = 0\n",
    "        self.dissimilar_verbs = 1\n",
    "        self.missing_dependencies = 2\n",
    "        self.contradiction = 3\n",
    "\n",
    "    # ==========================================\n",
    "    # 1) adding to KnowledgeGraph relations \n",
    "    # ==========================================\n",
    "    def add_verb(self, verb):\n",
    "        self.relations.append(self.get_relation(verb))\n",
    "\n",
    "        \n",
    "    ##### extracting relations from sentence #####\n",
    "    def get_relation(self, verb):\n",
    "        # get all equivalent verbs\n",
    "        verb_cluster = self.get_verb_cluster(verb)\n",
    "        actors = []\n",
    "        acteds = []\n",
    "        \n",
    "        # get all actors/acteds of verbs in equivalencies\n",
    "        for verb in verb_cluster:\n",
    "            actors += self.get_actors(verb)\n",
    "            acteds += self.get_acteds(verb)\n",
    "#         print(\"relation\", verb_cluster, actors, acteds)\n",
    "        return verb_cluster, actors, acteds\n",
    "    \n",
    "    # =========================================\n",
    "    # 2) looks through verb's children for\n",
    "    # verb equivalencies (xcomp)\n",
    "    # =========================================\n",
    "    def get_verb_cluster(self, verb):\n",
    "        verb_cluster = [verb]\n",
    "        for child in verb.children:\n",
    "            if child.dep_ == \"xcomp\":\n",
    "                verb_cluster.append(child)\n",
    "        return verb_cluster\n",
    "        \n",
    "    def get_actors(self, verb):\n",
    "        actors = []\n",
    "        for child in verb.children:\n",
    "            # child is a nominative subject\n",
    "            if child.dep_ == \"nsubj\":\n",
    "                actors.append(child)\n",
    "            # child is something like \"by\"\n",
    "            elif child.dep_ == \"agent\":  \n",
    "                # passive, look for true actor\n",
    "                for grandchild in child.children:\n",
    "                    if grandchild.dep_ == \"pobj\":\n",
    "                        actors.append(grandchild)\n",
    "        return actors\n",
    "\n",
    "    def get_acteds(self, verb):\n",
    "        acteds = []\n",
    "        for child in verb.children:\n",
    "            #child is direct object or passive subject\n",
    "            if child.dep_ == \"dobj\" or child.dep_ == \"nsubjpass\":\n",
    "                acteds.append(child)\n",
    "        return acteds\n",
    "\n",
    "    # =========================================\n",
    "    # 3) checking hypothesis relation against \n",
    "    # premise's KnowledgeGraph relations\n",
    "    # =========================================\n",
    "    def query_relation(self, hypothesis):\n",
    "        missing_dependencies = []\n",
    "        contradiction = []\n",
    "        for premise in self.relations:\n",
    "            r = self.implied_relation(premise, hypothesis)\n",
    "            \n",
    "            # once we find that hypothesis is contained,\n",
    "            # accept this relation as verified\n",
    "            if r[0] == self.entailment:\n",
    "                return r[0], [(premise, r[1])]\n",
    "            elif r[0] == self.missing_dependencies:\n",
    "                missing_dependencies.append((premise, r[1]))\n",
    "            elif r[0] == self.contradiction:\n",
    "                contradiction.append((premise, r[1]))\n",
    "        if len(contradiction) > 0:\n",
    "            return self.contradiction, contradiction\n",
    "        return self.missing_dependencies, missing_dependencies\n",
    "    \n",
    "    # check if a hypothesis is verified by a premise \n",
    "    # returns (result, proof)\n",
    "    def implied_relation(self, premise, hypothesis):\n",
    "        # premise[0] and hypothesis[0] is a list (verb cluster)\n",
    "        verb_similarity, best_pair = self.verb_same(premise[0], hypothesis[0])\n",
    "        if verb_similarity < self.verb_threshold:\n",
    "            return self.dissimilar_verbs, hypothesis\n",
    "\n",
    "        # check setminus of premise \\ hypothesis\n",
    "        actor_actor = self.noun_intersect_setminus(premise[1], hypothesis[1])\n",
    "        acted_acted = self.noun_intersect_setminus(premise[2], hypothesis[2])\n",
    "        actor_acted = self.noun_intersect_setminus(premise[1], hypothesis[2])\n",
    "        acted_actor = self.noun_intersect_setminus(premise[2], hypothesis[1])\n",
    "        contained_deps = actor_actor[0] + acted_acted[0]\n",
    "        missing_deps = actor_actor[1] + acted_acted[1]\n",
    "        contradiction_deps = actor_acted[0] + acted_actor[0]\n",
    "        if len(missing_deps) == 0:\n",
    "            return self.entailment, (\"verb similarity:\", verb_similarity,\n",
    "                    \"contained dependences:\", contained_deps)\n",
    "        if len(contradiction_deps) > 0:\n",
    "            return self.contradiction, (\"verb similarity:\", verb_similarity,\n",
    "                    \"contradictory dependences:\", contradiction_deps)\n",
    "        return self.missing_dependencies, (\"verb similarity:\",\n",
    "                verb_similarity, \"missing dependencies:\", missing_deps)\n",
    "\n",
    "    \n",
    "    # ========================\n",
    "    # verb helper functions\n",
    "    # ========================\n",
    "    # v1 comes from premise/source, v2 comes from hypothesis/output\n",
    "    def verb_same(self, v1_cluster, v2_cluster):\n",
    "        maximum_similarity = 0\n",
    "        maximum_pair = None\n",
    "        for v1 in v1_cluster:\n",
    "            for v2 in v2_cluster:\n",
    "                similarity = v1.similarity(v2)\n",
    "                if(similarity > maximum_similarity):\n",
    "                    maximum_similarity = similarity\n",
    "                    maximum_pair = v1, v2\n",
    "        return maximum_similarity, maximum_pair\n",
    "    \n",
    "    \n",
    "#     # returns (result, proof)\n",
    "#     def query_verb(self, verb):\n",
    "#         return self.query_relation(self.get_relation(verb))\n",
    "\n",
    "\n",
    "    # ========================\n",
    "    # noun helper functions\n",
    "    # ========================\n",
    "    def noun_intersect_setminus(self, supset, subset):\n",
    "        contained_nouns = []\n",
    "        missing_nouns = []\n",
    "        for n in subset:\n",
    "            contained = False\n",
    "            for n2 in supset:\n",
    "                r = self.noun_same(n, n2)\n",
    "                if r[0]:\n",
    "                    contained = True\n",
    "                    contained_nouns.append((n, n2, r[1]))\n",
    "                    continue\n",
    "            if not contained:\n",
    "                missing_nouns.append(n)\n",
    "        return contained_nouns, missing_nouns\n",
    "\n",
    "    def noun_same(self, n1, n2):\n",
    "        spans1 = self.get_valid_cluster_objects(n1)\n",
    "        spans2 = self.get_valid_cluster_objects(n2)\n",
    "        maximum_similarity = 0\n",
    "        maximum_pair = None\n",
    "        for span1 in spans1:\n",
    "            for span2 in spans2:\n",
    "                try:\n",
    "                    span_similarity = span1.similarity(span2)\n",
    "                except:\n",
    "                    continue\n",
    "                if span_similarity > maximum_similarity:\n",
    "                    maximum_similarity = span_similarity\n",
    "                    maximum_pair = span1, span2\n",
    "        if maximum_similarity > self.noun_threshold:\n",
    "            return True, (\"best match:\", maximum_similarity, maximum_pair)\n",
    "        return False, (\"best match:\", maximum_similarity, maximum_pair)\n",
    "\n",
    "    def get_valid_cluster_objects(self, noun):\n",
    "        spans = list()\n",
    "        spans.append(noun)\n",
    "        for cluster in noun._.coref_clusters:\n",
    "            for span in cluster:\n",
    "                if self.is_not_generic(span):\n",
    "                    spans.append(span)\n",
    "        return spans \n",
    "\n",
    "    def is_not_generic(self, span):\n",
    "        for word in span:\n",
    "            if word.pos_ != \"PRON\":\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(src, gen):\n",
    "#     print(\"source:\", src_line[:100])\n",
    "#     print(\"summary:\", gen_line[:100])\n",
    "    src = nlp(src)\n",
    "    gen = nlp(gen)\n",
    "#     print(\"clusters:\", src._.coref_clusters)\n",
    "    kg = KnowledgeGraph()\n",
    "\n",
    "    # put all actors/acteds for each verb into knowledge graph\n",
    "    for ixt, token in enumerate(src):\n",
    "        if token.pos_ == \"VERB\":\n",
    "            kg.add_verb(token)\n",
    "    important_relations = []\n",
    "    contained = 0\n",
    "    missing = 0\n",
    "    contradiction = 0\n",
    "    total = 0\n",
    "    \n",
    "    for token in gen:\n",
    "        # ignore xcomp verbs \"tried TO EAT\" since will later be added to verb cluster\n",
    "        # still adds was/has/is/aux verbs though\n",
    "        if token.pos_ == \"VERB\" and not(token.dep_=='xcomp'):\n",
    "            total += 1\n",
    "\n",
    "            relation = kg.get_relation(token)\n",
    "            r = kg.query_relation(relation)\n",
    "            if r[0] == kg.entailment:\n",
    "                contained += 1\n",
    "                important_relations.append(('contained', relation, r[1]))\n",
    "#                 print(\"contained |\", relation, \"|\", r[1])\n",
    "            elif r[0] == kg.missing_dependencies:\n",
    "                missing += 1\n",
    "                important_relations.append(('missing', relation, r[1]))\n",
    "#                 print(\"missing |\", relation, \"|\", r[1])\n",
    "            elif r[0] == kg.contradiction:\n",
    "                contradiction += 1\n",
    "                important_relations.append(('contradiction', relation, r[1]))\n",
    "#                 print(\"contradiction |\", relation, \"|\", r[1])\n",
    "    \n",
    "    important_relations = sorted(important_relations)\n",
    "    colored_src, colored_gen = visualize([word.text for word in src], [word.text for word in gen], important_relations)\n",
    "    \n",
    "    if total == 0:\n",
    "        return important_relations, (0.0, 0.0, 0.0), (colored_src, colored_gen)\n",
    "    return important_relations, (100.0 * contained / total, \n",
    "                                 100.0 * missing / total, \n",
    "                                 100.0 * contradiction / total), (colored_src, colored_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(src0, gen0, important_relations):\n",
    "    colors = {'contained':lambda text: '\\033[0;32m' + text + '\\033[0m', \n",
    "              'missing':lambda text: '\\033[0;33m' + text + '\\033[0m', \n",
    "              'contradiction':lambda text: '\\033[0;31m' + text + '\\033[0m'}\n",
    "\n",
    "    colored_src = src0\n",
    "    colored_gen = gen0\n",
    "    for relation_tuple in important_relations:\n",
    "        result = relation_tuple[0]\n",
    "        relation = relation_tuple[1]\n",
    "        proof = relation_tuple[2]\n",
    "        \n",
    "        # color output doc\n",
    "        verbs = relation[0]\n",
    "        actors = relation[1]\n",
    "        acteds = relation[2]\n",
    "        for verb in verbs:\n",
    "            colored_gen[verb.i] = colors[result](verb.text)\n",
    "        for a in actors:\n",
    "            colored_gen[a.i] = colors[result](a.text)\n",
    "        for a in acteds:\n",
    "            colored_gen[a.i] = colors[result](a.text)\n",
    "            \n",
    "        # color source doc\n",
    "        for p in proof:\n",
    "            for verb in p[0][0]:\n",
    "                colored_src[verb.i] = colors[result](verb.text)\n",
    "            for a in p[0][1]:\n",
    "                colored_src[a.i] = colors[result](a.text)\n",
    "            for a in p[0][2]:\n",
    "                colored_src[a.i] = colors[result](a.text)\n",
    "\n",
    "    colored_src = ' '.join(colored_src)\n",
    "    colored_gen = ' '.join(colored_gen)\n",
    "\n",
    "    return colored_src, colored_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns average copy length = max copy length / unique phrases copied\n",
    "def avg_copy_length(src,gen):\n",
    "    src = src.split()\n",
    "    gen = gen.split()\n",
    "    substrings = {}\n",
    "    for ixgw,word in enumerate(gen):\n",
    "        substrings[ixgw] = []\n",
    "    \n",
    "    avg_length = 0\n",
    "    num_copied = 0\n",
    "    ixgw = 0\n",
    "    while(ixgw < len(gen)):\n",
    "        gen_word = gen[ixgw]\n",
    "#         print(gen_word)\n",
    "        max_j = 0\n",
    "        src_ix = -1\n",
    "        for ixsw, src_word in enumerate(src):\n",
    "            j = 0\n",
    "            while(ixgw+j < len(gen) and ixsw+j < len(src) and src[ixsw:ixsw+j] == gen[ixgw:ixgw+j]):\n",
    "                j += 1\n",
    "            if(j > max_j):\n",
    "                max_j = j\n",
    "                src_ix = ixsw\n",
    "        substrings[ixgw] = (gen[ixgw:ixgw+max_j-1], src_ix)\n",
    "        ixgw += 1\n",
    "        \n",
    "    for ixgw,gen_word in enumerate(gen):\n",
    "        substr = substrings[ixgw][0]\n",
    "        src_ix = substrings[ixgw][1]\n",
    "        if not (ixgw > 0 and src_ix-1 == substrings[ixgw-1][1]):\n",
    "            if(len(substrings[ixgw][0])>0):\n",
    "                num_copied += 1\n",
    "                print(substrings[ixgw])\n",
    "                avg_length += len(substr)\n",
    "    avg_length /= num_copied\n",
    "    \n",
    "    return avg_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src 13: jeremy clarkson has finally spoken out about his high - profile sacking from top gear , admitting he will ` miss ' fronting the bbc2 show . clarkson , 54 , \u001b[0;33mwho\u001b[0m \u001b[0;32mwas\u001b[0m \u001b[0;33mfired\u001b[0m in march after \u001b[0;32mattacking\u001b[0m a \u001b[0;32mproducer\u001b[0m during filming , has not publicly addressed his dismissal , apart from to say that ` everyone 's upset ' . but the presenter today gave his ` heartfelt thanks ' to those who sent their support in the aftermath of his sacking , saying he would ` miss being there ' . it came as ms rippon , 70 , \u001b[0;33mwho\u001b[0m first \u001b[0;33mpresented\u001b[0m the \u001b[0;33mshow\u001b[0m 38 years ago , \u001b[0;32msaid\u001b[0m \u001b[0;32mshe\u001b[0m would \u001b[0;32mbe\u001b[0m keen to return to the motoring show if \u001b[0;32mshe\u001b[0m \u001b[0;32mwere\u001b[0m \u001b[0;32minvited\u001b[0m back . scroll down for video . jeremy clarkson today revealed he would ` miss ' fronting top gear , as it emerged that former presenter angela rippon would be keen to return to the motoring show . ms rippon , 70 , \u001b[0;33mwho\u001b[0m first \u001b[0;33mpresented\u001b[0m the \u001b[0;33mshow\u001b[0m 38 years ago , said it would be ` great ' if she were invited back to front top gear . writing in the sun , clarkson said : ` heartfelt thanks to all those who have written to say how much they will miss me on top gear . it 's not as much , however , as i 'll miss being there . ' clarkson was dropped by the bbc after an internal investigation found he had launched an ` unprovoked ' 30-second physical attack on producer oisin tymon because he was offered a plate of cold cuts instead of steak and chips . the presenter was found to have split the producer 's lip and verbally abused him . following weeks of speculation over the star 's future , lord hall , the director - general of the corporation , publicly announced that the bbc would not be renewing clarkson 's contract . he said clarkson had ` crossed the line ' and that ` there can not be one rule for one and one rule for another ' . now , with the future of the show still up in the air , former newsreader ms \u001b[0;32mrippon\u001b[0m \u001b[0;32msaid\u001b[0m it would be ` great ' if she was invited back . clarkson , who was dropped by the bbc after launching an\n",
      "summary 13: \u001b[0;33mclarkson\u001b[0m , 54 , \u001b[0;32mwas\u001b[0m \u001b[0;33mfired\u001b[0m in march after \u001b[0;32mattacking\u001b[0m \u001b[0;32mproducer\u001b[0m oisin tymon . ms \u001b[0;32mrippon\u001b[0m \u001b[0;32msaid\u001b[0m \u001b[0;32mit\u001b[0m would \u001b[0;32mbe\u001b[0m ` great ' if \u001b[0;32mshe\u001b[0m \u001b[0;32mwere\u001b[0m \u001b[0;32minvited\u001b[0m back to front top gear . ms \u001b[0;33mrippon\u001b[0m , 70 , first \u001b[0;33mpresented\u001b[0m the \u001b[0;33mshow\u001b[0m 38 years ago , \u001b[0;32msaid\u001b[0m \u001b[0;32mshe\u001b[0m would \u001b[0;32mbe\u001b[0m keen to return to the motoring show .\n",
      "score: (80.0, 20.0, 0.0)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-497-bdfae1e27d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"summary {i}:\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolored_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mavg_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_copy_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avg copy length:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"===========================================================================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-494-9675f305cc7c>\u001b[0m in \u001b[0;36mavg_copy_length\u001b[0;34m(src, gen)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# returns average copy length = max copy length / unique phrases copied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mavg_copy_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msubstrings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "line_num = 0\n",
    "scores = []\n",
    "src_lines = []\n",
    "gen_lines = []\n",
    "with open(\"data/test.txt.src.tagged.shuf.400words\") as src:\n",
    "    with open(\"data/bottom_up_cnndm_015_threshold.out\") as gen:\n",
    "        for i, (src_line, gen_line) in enumerate(zip(src, gen)):\n",
    "            if i <=12:\n",
    "                continue\n",
    "            if line_num > 0 and not i == line_num:\n",
    "                continue\n",
    "            if line_num == 0 and i >= 14:\n",
    "                break\n",
    "            src_line = clean_src(src_line)\n",
    "            src_lines.append(src_line)\n",
    "            gen_line = clean_gen(gen_line)\n",
    "            gen_lines.append(gen_line)\n",
    "            important_relations, score, (colored_src, colored_gen) = test(src_line, gen_line)\n",
    "            print(f\"src {i}:\"%{i:i}, colored_src)\n",
    "            print(f\"summary {i}:\"%{i:i}, colored_gen)\n",
    "            print(\"score:\", score)\n",
    "            avg_length = avg_copy_length(src, gen)\n",
    "            print(\"avg copy length:\", avg_length)\n",
    "            print(\"===========================================================================================\")\n",
    "            scores.append(score)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in doc:\n",
    "#     print(word.i, word, word.dep_, word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(src_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jeremy compound clarkson\n",
      "clarkson nsubj spoken\n",
      "has aux spoken\n",
      "finally advmod spoken\n",
      "spoken ROOT spoken\n",
      "out prt spoken\n",
      "about prep spoken\n",
      "his poss sacking\n",
      "high amod profile\n",
      "- punct profile\n",
      "profile compound sacking\n",
      "sacking pobj about\n",
      "from prep sacking\n",
      "top amod gear\n",
      "gear pobj from\n",
      ", punct spoken\n",
      "admitting advcl spoken\n",
      "he nsubj miss\n",
      "will aux miss\n",
      "` punct miss\n",
      "miss ccomp admitting\n",
      "' punct miss\n",
      "fronting advcl miss\n",
      "the det show\n",
      "bbc2 compound show\n",
      "show dobj fronting\n",
      ". punct spoken\n",
      "clarkson nsubj addressed\n",
      ", punct clarkson\n",
      "54 appos clarkson\n",
      ", punct clarkson\n",
      "who nsubjpass fired\n",
      "was auxpass fired\n",
      "fired relcl clarkson\n",
      "in prep fired\n",
      "march pobj in\n",
      "after prep fired\n",
      "attacking pcomp after\n",
      "a det producer\n",
      "producer dobj attacking\n",
      "during prep attacking\n",
      "filming pobj during\n",
      ", punct clarkson\n",
      "has aux addressed\n",
      "not neg addressed\n",
      "publicly advmod addressed\n",
      "addressed ROOT addressed\n",
      "his poss dismissal\n",
      "dismissal dobj addressed\n",
      ", punct addressed\n",
      "apart advmod from\n",
      "from prep addressed\n",
      "to aux say\n",
      "say xcomp from\n",
      "that mark upset\n",
      "` punct upset\n",
      "everyone poss upset\n",
      "'s case everyone\n",
      "upset ccomp say\n",
      "' punct addressed\n",
      ". punct addressed\n",
      "but cc gave\n",
      "the det presenter\n",
      "presenter nsubj gave\n",
      "today npadvmod gave\n",
      "gave ROOT gave\n",
      "his poss thanks\n",
      "` punct heartfelt\n",
      "heartfelt amod thanks\n",
      "thanks dobj gave\n",
      "' punct gave\n",
      "to prep gave\n",
      "those pobj to\n",
      "who nsubj sent\n",
      "sent relcl those\n",
      "their poss support\n",
      "support dobj sent\n",
      "in prep sent\n",
      "the det aftermath\n",
      "aftermath pobj in\n",
      "of prep aftermath\n",
      "his poss sacking\n",
      "sacking pobj of\n",
      ", punct gave\n",
      "saying advcl gave\n",
      "he nsubj miss\n",
      "would aux miss\n",
      "` punct miss\n",
      "miss ccomp saying\n",
      "being xcomp miss\n",
      "there advmod being\n",
      "' punct saying\n",
      ". punct gave\n",
      "it nsubj came\n",
      "came ccomp said\n",
      "as mark rippon\n",
      "ms nsubj rippon\n",
      "rippon advcl came\n",
      ", punct rippon\n",
      "70 npadvmod rippon\n",
      ", punct rippon\n",
      "who nsubj presented\n",
      "first advmod presented\n",
      "presented relcl rippon\n",
      "the det show\n",
      "show dobj presented\n",
      "38 nummod years\n",
      "years npadvmod ago\n",
      "ago advmod presented\n",
      ", punct said\n",
      "said ROOT said\n",
      "she nsubj be\n",
      "would aux be\n",
      "be ccomp said\n",
      "keen acomp be\n",
      "to aux return\n",
      "return xcomp keen\n",
      "to prep return\n",
      "the det show\n",
      "motoring compound show\n",
      "show pobj to\n",
      "if mark invited\n",
      "she nsubjpass invited\n",
      "were auxpass invited\n",
      "invited advcl be\n",
      "back advmod invited\n",
      ". punct said\n",
      "scroll ROOT scroll\n",
      "down prt scroll\n",
      "for prep scroll\n",
      "video pobj for\n",
      ". punct scroll\n",
      "jeremy compound clarkson\n",
      "clarkson nsubj revealed\n",
      "today npadvmod revealed\n",
      "revealed ROOT revealed\n",
      "he nsubj miss\n",
      "would aux miss\n",
      "` punct miss\n",
      "miss ccomp revealed\n",
      "' punct miss\n",
      "fronting xcomp miss\n",
      "top amod gear\n",
      "gear dobj fronting\n",
      ", punct miss\n",
      "as mark emerged\n",
      "it nsubj emerged\n",
      "emerged advcl miss\n",
      "that mark be\n",
      "former amod presenter\n",
      "presenter compound rippon\n",
      "angela compound rippon\n",
      "rippon nsubj be\n",
      "would aux be\n",
      "be ccomp emerged\n",
      "keen acomp be\n",
      "to aux return\n",
      "return xcomp keen\n",
      "to prep return\n",
      "the det show\n",
      "motoring compound show\n",
      "show pobj to\n",
      ". punct revealed\n",
      "ms nsubj rippon\n",
      "rippon ccomp said\n",
      ", punct rippon\n",
      "70 npadvmod rippon\n",
      ", punct rippon\n",
      "who nsubj presented\n",
      "first advmod presented\n",
      "presented relcl rippon\n",
      "the det show\n",
      "show dobj presented\n",
      "38 nummod years\n",
      "years npadvmod ago\n",
      "ago advmod presented\n",
      ", punct said\n",
      "said ROOT said\n",
      "it nsubj be\n",
      "would aux be\n",
      "be ccomp said\n",
      "` punct great\n",
      "great acomp be\n",
      "' punct great\n",
      "if mark invited\n",
      "she nsubjpass invited\n",
      "were auxpass invited\n",
      "invited advcl be\n",
      "back advmod invited\n",
      "to prep back\n",
      "front amod gear\n",
      "top amod gear\n",
      "gear pobj to\n",
      ". punct said\n",
      "writing advcl said\n",
      "in prep writing\n",
      "the det sun\n",
      "sun pobj in\n",
      ", punct said\n",
      "clarkson nsubj said\n",
      "said ROOT said\n",
      ": punct said\n",
      "` punct heartfelt\n",
      "heartfelt xcomp said\n",
      "thanks dobj heartfelt\n",
      "to prep thanks\n",
      "all predet those\n",
      "those pobj to\n",
      "who nsubj written\n",
      "have aux written\n",
      "written relcl those\n",
      "to aux say\n",
      "say xcomp written\n",
      "how advmod much\n",
      "much dative miss\n",
      "they nsubj miss\n",
      "will aux miss\n",
      "miss ccomp say\n",
      "me dobj miss\n",
      "on prep miss\n",
      "top amod gear\n",
      "gear pobj on\n",
      ". punct said\n",
      "it nsubj 's\n",
      "'s ROOT 's\n",
      "not neg 's\n",
      "as advmod much\n",
      "much attr 's\n",
      ", punct 's\n",
      "however advmod 's\n",
      ", punct 's\n",
      "as mark miss\n",
      "i nsubj miss\n",
      "'ll aux miss\n",
      "miss advcl 's\n",
      "being xcomp miss\n",
      "there advmod being\n",
      ". punct 's\n",
      "' punct 's\n",
      "clarkson nsubjpass dropped\n",
      "was auxpass dropped\n",
      "dropped ROOT dropped\n",
      "by agent dropped\n",
      "the det bbc\n",
      "bbc pobj by\n",
      "after mark found\n",
      "an det investigation\n",
      "internal amod investigation\n",
      "investigation nsubj found\n",
      "found advcl dropped\n",
      "he nsubj launched\n",
      "had aux launched\n",
      "launched ccomp found\n",
      "an det attack\n",
      "` punct attack\n",
      "unprovoked amod attack\n",
      "' punct attack\n",
      "30-second nmod attack\n",
      "physical amod attack\n",
      "attack dobj launched\n",
      "on prep attack\n",
      "producer compound oisin\n",
      "oisin compound tymon\n",
      "tymon pobj on\n",
      "because mark offered\n",
      "he nsubjpass offered\n",
      "was auxpass offered\n",
      "offered advcl launched\n",
      "a det plate\n",
      "plate dobj offered\n",
      "of prep plate\n",
      "cold amod cuts\n",
      "cuts pobj of\n",
      "instead advmod of\n",
      "of cc cuts\n",
      "steak pobj of\n",
      "and cc steak\n",
      "chips conj steak\n",
      ". punct dropped\n",
      "the det presenter\n",
      "presenter nsubjpass found\n",
      "was auxpass found\n",
      "found ROOT found\n",
      "to aux split\n",
      "have aux split\n",
      "split xcomp found\n",
      "the det producer\n",
      "producer poss lip\n",
      "'s case producer\n",
      "lip dobj split\n",
      "and cc split\n",
      "verbally advmod abused\n",
      "abused conj split\n",
      "him dobj abused\n",
      ". punct found\n",
      "following prep announced\n",
      "weeks pobj following\n",
      "of prep weeks\n",
      "speculation pobj of\n",
      "over prep speculation\n",
      "the det star\n",
      "star poss future\n",
      "'s case star\n",
      "future pobj over\n",
      ", punct announced\n",
      "lord compound hall\n",
      "hall pobj ,\n",
      ", punct hall\n",
      "the det director\n",
      "director appos hall\n",
      "- punct director\n",
      "general amod director\n",
      "of prep general\n",
      "the det corporation\n",
      "corporation pobj of\n",
      ", punct announced\n",
      "publicly advmod announced\n",
      "announced ROOT announced\n",
      "that mark renewing\n",
      "the det bbc\n",
      "bbc nsubj renewing\n",
      "would aux renewing\n",
      "not neg renewing\n",
      "be aux renewing\n",
      "renewing ccomp announced\n",
      "clarkson poss contract\n",
      "'s case clarkson\n",
      "contract dobj renewing\n",
      ". punct announced\n",
      "he nsubj said\n",
      "said ROOT said\n",
      "clarkson nsubj crossed\n",
      "had aux crossed\n",
      "` punct crossed\n",
      "crossed ccomp said\n",
      "the det line\n",
      "line dobj crossed\n",
      "' punct crossed\n",
      "and cc crossed\n",
      "that mark be\n",
      "` punct be\n",
      "there expl be\n",
      "can aux be\n",
      "not neg be\n",
      "be conj crossed\n",
      "one nummod rule\n",
      "rule attr be\n",
      "for prep rule\n",
      "one nummod rule\n",
      "and cc one\n",
      "one conj one\n",
      "rule pobj for\n",
      "for prep rule\n",
      "another pobj for\n",
      "' punct be\n",
      ". punct said\n",
      "now advmod said\n",
      ", punct said\n",
      "with prep said\n",
      "the det future\n",
      "future nsubj up\n",
      "of prep future\n",
      "the det show\n",
      "show pobj of\n",
      "still advmod up\n",
      "up pcomp with\n",
      "in prep up\n",
      "the det air\n",
      "air pobj in\n",
      ", punct said\n",
      "former amod rippon\n",
      "newsreader compound ms\n",
      "ms compound rippon\n",
      "rippon nsubj said\n",
      "said ROOT said\n",
      "it nsubj be\n",
      "would aux be\n",
      "be ccomp said\n",
      "` punct great\n",
      "great acomp be\n",
      "' punct great\n",
      "if mark invited\n",
      "she nsubjpass invited\n",
      "was auxpass invited\n",
      "invited advcl be\n",
      "back advmod invited\n",
      ". punct said\n",
      "clarkson ROOT clarkson\n",
      ", punct clarkson\n",
      "who nsubjpass dropped\n",
      "was auxpass dropped\n",
      "dropped relcl clarkson\n",
      "by agent dropped\n",
      "the det bbc\n",
      "bbc pobj by\n",
      "after prep dropped\n",
      "launching pcomp after\n",
      "an dobj launching\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.dep_, token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_lines[0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.load(\"sandbox/scores.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(scores[:,0], scores[:,2], marker='.')\n",
    "plt.xlabel(\"% contained\")\n",
    "plt.ylabel(\"% contradiction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying BERT embeddings...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relations between iran and saudi arabia have always been thorny , but rarely has the state of affairs been as venomous as it is today',\n",
       " 'tehran and riyadh each point to the other as the main reason for much of the turmoil in the middle east',\n",
       " 'in its most recent incarnation , the iranian-saudi conflict by proxy has reached yemen in a spiral that both sides portray as climatic',\n",
       " \"for riyadh and its regional allies , the saudi military intervention in yemen -- `` operation decisive storm'' -- is the moment the sunni arab nation finally woke up to repel the expansion of shia-iranian influence\",\n",
       " \"for tehran and its regional allies -- including the houthi movement in yemen -- saudi arabia's actions are in defense of a retrogressive status quo order that is no longer tenable\",\n",
       " 'and yet both sides have good reasons to want to stop the yemeni crisis from spiraling out of control and evolving into an unwinnable war',\n",
       " 'when iranian president hassan rouhani was elected in june 2013 , he pledged to reach out to riyadh',\n",
       " \"he was up front and called tehran's steep deterioration of relations with the saudis over the last decade as one of the principal burdens on iranian foreign policy\",\n",
       " 'from lebanon and afghanistan to pakistan and the gaza strip , the iranian-saudi rivalry and conflict through proxy has been deep and costly',\n",
       " \"and yet despite rouhani's open pledge , profound differences over syria and iraq in particular have kept riyadh and tehran apart\",\n",
       " 'but if the questions of syria and iraq prevented a pause in hostilities , the saudi military intervention in yemen since late march has all but raised the stakes to unprecedentedly dangerous levels',\n",
       " 'unlike in syria and in iraq , the saudi military is now directly battling it out with iranian-backed rebels in yemen',\n",
       " \"while riyadh no doubt exaggerates tehran's role in the yemen crisis , its fingerprints are nonetheless evident\",\n",
       " \"`` iran provides financial support , weapons , training and intelligence to houthis ,'' gerald feierstein , a u.s. state department official and former yemen ambassador , told a congressional hearing last week\",\n",
       " '`` we believe that iran sees opportunities with the houthis to expand its influence in yemen and threaten saudi and gulf arab']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embedding = BertEmbedding()\n",
    "embed = bert_embedding(src_lines[0].split(' . '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relations',\n",
       " 'between',\n",
       " 'iran',\n",
       " 'and',\n",
       " 'saudi',\n",
       " 'arabia',\n",
       " 'have',\n",
       " 'always',\n",
       " 'been',\n",
       " 'thorny',\n",
       " ',',\n",
       " 'but',\n",
       " 'rarely',\n",
       " 'has',\n",
       " 'the',\n",
       " 'state',\n",
       " 'of',\n",
       " 'affairs',\n",
       " 'been',\n",
       " 'as',\n",
       " 'venomous']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(embed[0][1])):\n",
    "#     plt.scatter([embed[0][1][i][0]], [embed[0][1][i][1]], label=embed[0][0][i])\n",
    "# # plt.scatter([embed[0][1][i][0] for i in range(len(embed[0][1]))], \n",
    "# #             [embed[0][1][i][1] for i in range(len(embed[0][1]))])\n",
    "# # plt.scatter([embed[0][1][2][0]], [embed[0][1][2][1]])\n",
    "# # plt.scatter([embed[0][1][4][0]], [embed[0][1][4][1]])\n",
    "# # plt.scatter([embed[0][1][5][0]], [embed[0][1][5][1]])\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.376158"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(embed[0][1][4] - embed[0][1][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab file is not found. Downloading.\n",
      "Downloading /Users/cliang/.mxnet/models/book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
      "Downloading /Users/cliang/.mxnet/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip...\n"
     ]
    }
   ],
   "source": [
    "bert_embedding = BertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "with open(\"data/test.txt.src.tagged.shuf.400words\") as src:\n",
    "    with open(\"data/bottom_up_cnndm_015_threshold.out\") as gen:\n",
    "        for src_line0, gen_line0 in zip(src, gen):\n",
    "            if(i >= 8):\n",
    "                break\n",
    "#             print(src_line0)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = gen_line.split(' . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = bert_embedding([\"I ate a dog\", \"I ate a cat\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'ate', 'a', 'dog']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.3448873"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(result[1][1][3]- result[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9955516"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(result[1][1][1]- result[0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
